$schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json
name: text_classification_finetune
version: 0.0.5
type: command

is_deterministic: True

display_name: Text Classification Finetune
description: Component to finetune model for single label classification task

environment: azureml://registries/azureml/environments/acft-hf-nlp-gpu/versions/3

code: ../../../src/finetune

distribution:
  type: pytorch

inputs:
  # Lora parameters
  apply_lora:
    type: string
    enum:
      - "true"
      - "false"
    default: "false"
    optional: true
    description:
      If "true" enables lora. The default is "false". The lora is `ONLY` supported for following model families -
      1. GPT2
      2. BERT
      3. ROBERTA
      4. DEBERTA
      5. DISTILBERT
      6. T5
      7. BART
      8. MBART
      9. CAMEMBERT

      NOTE When *finetune component* is connected to *model_evaluation* component, _merge_lora_weights_ **MUST** be set to "true" when _apply_lora_ is "true"

  merge_lora_weights:
    type: string
    enum:
      - "true"
      - "false"
    default: "true"
    optional: true
    description: If "true", the lora weights are merged with the base Hugging Face model. The default value is "true"

  lora_alpha:
    type: integer
    default: 128
    optional: true
    description: alpha attention parameter for lora. The default value is 128

  lora_r:
    type: integer
    default: 8
    optional: true
    description: The rank to be used with lora. The default value is 8

  lora_dropout:
    type: number
    default: 0.0
    optional: true
    description: lora dropout value. The default value is 0.0

  # Training parameters
  num_train_epochs:
    type: integer
    default: 1
    optional: true
    description: Number of epochs to run for finetune. The default value is 5

  max_steps:
    type: integer
    default: -1
    optional: true
    description: If set to a positive number, the total number of training steps to perform. Overrides `num_train_epochs`. In case of using a finite iterable dataset the training may stop before reaching the set number of steps when all data is exhausted. default value is -1

  per_device_train_batch_size:
    type: integer
    default: 1
    optional: true
    description: Per device batch size used for training. The default value is 1. However, the effective batch size is per_device_train_batch_size x number of gpus per device x number of nodes

  per_device_eval_batch_size:
    type: integer
    default: 1
    optional: true
    description: Per device batch size used for validation. The default value is 1. However, the effective batch size is per_device_eval_batch_size x number of gpus per device x number of nodes

  auto_find_batch_size:
    type: string
    enum:
      - "true"
      - "false"
    default: "false"
    optional: true
    description: If set to "true", the train batch size will be automatically downscaled recursively till if finds a valid batch size that fits into memory. The default value is "false".

  optim:
    type: string
    default: adamw_hf
    optional: true
    enum:
      - adamw_hf
      - adamw_torch
      # - adamw_apex_fused
      - adafactor
    description: Optimizer to be used while training. The default value is "adamw_hf". The other available optimizers are
      - adamw_hf
      - adamw_torch
      - adafactor

  learning_rate:
    type: number
    default: 0.00002
    optional: true
    description: Start learning rate used for training. The default value is 2e-5

  warmup_steps:
    type: integer
    default: 0
    optional: true
    description: The number of steps for the learning rate scheduler warmup phase. The default value is 0

  weight_decay:
    type: number
    default: 0.0
    optional: true
    description: If not 0, the weight decay will be applied to all layers except all bias and LayerNorm weights in AdamW optimizer. The default value is 0

  adam_beta1:
    type: number
    default: 0.9
    optional: true
    description: The beta1 hyperparameter for the AdamW optimizer. The default value is 0.9

  adam_beta2:
    type: number
    default: 0.999
    optional: true
    description: The beta2 hyperparameter for the AdamW optimizer. The default value is 0.999

  adam_epsilon:
    type: number
    default: 1e-8
    optional: true
    description: The epsilon hyperparameter for the AdamW optimizer. The default value is 1e-8

  gradient_accumulation_steps:
    type: integer
    default: 1
    optional: true
    description: Number of updates steps to accumulate the gradients for, before performing a backward/update pass. The default value is 1

  lr_scheduler_type:
    type: string
    default: linear
    optional: true
    enum:
      - linear
      - cosine
      - cosine_with_restarts
      - polynomial
      - constant
      - constant_with_warmup
    description: The learning rate scheduler to use. The default value is `linear`. The other available schedulers are
      - linear
      - cosine
      - cosine_with_restarts
      - polynomial
      - constant
      - constant_with_warmup

  precision:
    type: string
    enum:
      - "32"
      - "16"
    default: "32"
    optional: true
    description: Apply mixed precision training. This can reduce memory footprint by performing operations in half-precision. The supported precision values are 16 and 32. The default value is 32

  seed:
    type: integer
    default: 42
    optional: true
    description: Random seed that will be set at the beginning of training. The default value is 42

  enable_full_determinism:
    type: string
    enum:
      - "true"
      - "false"
    default: "false"
    optional: true
    description: Ensure reproducible behavior during distributed training

  dataloader_num_workers:
    type: integer
    default: 0
    optional: true
    description: Number of subprocesses to use for data loading. The default value is 0 which means that the data will be loaded in the main process.

  ignore_mismatched_sizes:
    type: string
    enum:
      - "true"
      - "false"
    default: "true"
    optional: true
    description: Not setting this flag will raise an error if some of the weights from the checkpoint do not have the same size as the weights of the model. The default value is `true`.

  max_grad_norm:
    type: number
    default: 1.0
    optional: true
    description: Maximum gradient norm (for gradient clipping). The default value is 1.0.

  evaluation_strategy:
    type: string
    default: epoch
    optional: true
    enum:
      - epoch
      - steps
    description: The evaluation strategy to adopt during training. If set to "steps", either the `evaluation_steps_interval` or `eval_steps` needs to be specified, which helps to determine the step at which the model evaluation needs to be computed else evaluation happens at end of each epoch. The default value is "epoch"

  evaluation_steps_interval:
    type: number
    default: 0.0
    optional: true
    description: The evaluation steps in fraction of an epoch steps to adopt during training. Overwrites eval_steps if not 0. The default value is 0

  eval_steps:
    type: integer
    default: 500
    optional: true
    description: Number of update steps between two model evaluations if evaluation_strategy='steps'. The default value is 500

  logging_strategy:
    type: string
    default: epoch
    optional: true
    enum:
      - epoch
      - steps
    description: The logging strategy to adopt during training. If set to "steps", the `logging_steps` will decide the frequency of logging else logging happens at the end of epoch. The default value is "epoch".

  logging_steps:
    type: integer
    default: 500
    optional: true
    description: Number of update steps between two logs if logging_strategy='steps'. The default value is 100

  metric_for_best_model:
    type: string
    default: loss
    optional: true
    enum:
      - loss
      - f1_macro
      - mcc
      - accuracy
      - precision_macro
      - recall_macro
    description: Specify the metric to use to compare two different models

  resume_from_checkpoint:
    type: string
    default: "false"
    optional: true
    enum:
      - "true"
      - "false"
    description: If set to "true", resumes the training from last saved checkpoint. Along with loading the saved weights, saved optimizer, scheduler and random states will be loaded if exists. The default value is "false"

  save_total_limit:
    type: integer
    default: -1
    optional: true
    description: If a value is passed, will limit the total amount of checkpoints. Deletes the older checkpoints in output_dir. If the value is -1 saves all checkpoints". The default value is -1

  # Early Stopping Parameters
  apply_early_stopping:
    type: string
    default: "false"
    optional: true
    enum:
      - "true"
      - "false"
    description: If set to "true", early stopping is enabled. The default value is "false"

  early_stopping_patience:
    type: integer
    default: 1
    optional: true
    description: Stop training when the specified metric worsens for early_stopping_patience evaluation calls.

  early_stopping_threshold:
    type: number
    default: 0.0
    optional: true
    description: Denotes how much the specified metric must improve to satisfy early stopping conditions.

  # Deepspeed Parameters
  apply_deepspeed:
    type: string
    enum:
      - "true"
      - "false"
    default: "false"
    optional: true
    description: If set to true, will enable deepspeed for training

  deepspeed:
    type: uri_file
    optional: true
    description: Input path to the deepspeed config file. This is a JSON file that can be used to configure optimizer, scheduler, batch size and other training related parameters. This is the [default deepspeed config](https://aka.ms/azureml-ft-docs-sample-deepspeed-config) used when [apply_deepspeed](#33-deepspeed-and-ort-parameters) is set to `true`. Alternatively, you can pass your custom deepspeed config. Please follow the [deepspeed docs](https://www.deepspeed.ai/docs/config-json/) to create the custom config.

  # ORT Parameters
  apply_ort:
    type: string
    enum:
      - "true"
      - "false"
    default: "false"
    optional: true
    description: If set to true, will use the ONNXRunTime training

  # MLFlow Parameters
  save_as_mlflow_model:
    type: string
    enum:
      - "true"
      - "false"
    default: "true"
    optional: true
    description: If set to true, will save as mlflow model which can be later registered in the local workspace

  # Dataset parameters
  preprocess_output:
    type: uri_folder
    optional: false
    description: output folder of preprocessor containing encoded data, raw data and the tokenizer config

  model_selector_output:
    type: uri_folder
    optional: false
    description: output folder of model selector containing model metadata like config, checkpoints, tokenizer config

outputs:
  pytorch_model_folder:
    type: uri_folder
    description: Output dir to save the finetune model and other metadata

  mlflow_model_folder:
    type: mlflow_model
    description: Output dir to save the finetune model as mlflow model

command: >-
  python finetune.py $[[--apply_lora ${{inputs.apply_lora}}]] $[[--merge_lora_weights ${{inputs.merge_lora_weights}}]] $[[--lora_alpha ${{inputs.lora_alpha}}]] $[[--lora_r ${{inputs.lora_r}}]] $[[--lora_dropout ${{inputs.lora_dropout}}]] $[[--num_train_epochs ${{inputs.num_train_epochs}}]] $[[--max_steps ${{inputs.max_steps}}]] $[[--per_device_train_batch_size ${{inputs.per_device_train_batch_size}}]] $[[--per_device_eval_batch_size ${{inputs.per_device_eval_batch_size}}]] $[[--auto_find_batch_size ${{inputs.auto_find_batch_size}}]] $[[--optim ${{inputs.optim}}]] $[[--learning_rate ${{inputs.learning_rate}}]] $[[--warmup_steps ${{inputs.warmup_steps}}]] $[[--weight_decay ${{inputs.weight_decay}}]] $[[--adam_beta1 ${{inputs.adam_beta1}}]] $[[--adam_beta2 ${{inputs.adam_beta2}}]] $[[--adam_epsilon ${{inputs.adam_epsilon}}]] $[[--gradient_accumulation_steps ${{inputs.gradient_accumulation_steps}}]] $[[--lr_scheduler_type ${{inputs.lr_scheduler_type}}]] $[[--precision ${{inputs.precision}}]] $[[--seed ${{inputs.seed}}]] $[[--enable_full_determinism ${{inputs.enable_full_determinism}}]] $[[--dataloader_num_workers ${{inputs.dataloader_num_workers}}]] $[[--ignore_mismatched_sizes ${{inputs.ignore_mismatched_sizes}}]] $[[--max_grad_norm ${{inputs.max_grad_norm}}]] $[[--evaluation_strategy ${{inputs.evaluation_strategy}}]] $[[--evaluation_steps_interval ${{inputs.evaluation_steps_interval}}]] $[[--eval_steps ${{inputs.eval_steps}}]] $[[--logging_strategy ${{inputs.logging_strategy}}]] $[[--logging_steps ${{inputs.logging_steps}}]] $[[--metric_for_best_model ${{inputs.metric_for_best_model}}]] $[[--resume_from_checkpoint ${{inputs.resume_from_checkpoint}}]] $[[--save_total_limit ${{inputs.save_total_limit}}]] $[[--apply_early_stopping ${{inputs.apply_early_stopping}}]] $[[--early_stopping_patience ${{inputs.early_stopping_patience}}]] $[[--early_stopping_threshold ${{inputs.early_stopping_threshold}}]] $[[--apply_ort ${{inputs.apply_ort}}]] $[[--apply_deepspeed ${{inputs.apply_deepspeed}}]] $[[--deepspeed '${{inputs.deepspeed}}']] $[[--save_as_mlflow_model ${{inputs.save_as_mlflow_model}}]] --model_selector_output ${{inputs.model_selector_output}} --preprocess_output ${{inputs.preprocess_output}} --pytorch_model_folder ${{outputs.pytorch_model_folder}} --mlflow_model_folder ${{outputs.mlflow_model_folder}}
